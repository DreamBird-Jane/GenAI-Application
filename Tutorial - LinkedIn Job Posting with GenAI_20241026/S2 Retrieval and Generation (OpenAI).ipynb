{"cells":[{"source":"# Retrieval and Generation\n\n**Vector Database (Vector DB)**\nResources \n- [How-to guides](https://python.langchain.com/v0.2/docs/how_to/#vector-stores)\n  - [Vectorstores](https://python.langchain.com/v0.2/docs/integrations/vectorstores/): A vector store that stores embedded data and performs similarity search.\n    1. [Elasticsearch](https://python.langchain.com/v0.2/docs/integrations/vectorstores/elasticsearch/)\n    2. [Milvus](https://python.langchain.com/v0.2/docs/integrations/vectorstores/milvus/)\n    3. [Chroma](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)","metadata":{},"id":"3134fe35-1ec3-4179-8f02-1e49de949f5b","cell_type":"markdown"},{"source":"# Preface\n## Environment Setup\n\nSources  \n- [langchain-chroma](https://pypi.org/project/langchain-chroma/)","metadata":{},"id":"2b4d9104-6cbd-475d-ab1d-f2faf8f636bc","cell_type":"markdown"},{"source":"from importlib.metadata import version\n# #!pip install langchain\n# # Select langchain to 0.1.3\n# try:\n#     assert version('langchain') == '0.1.20'\n# except:\n#     !pip install langchain==0.1.20\n# print('langchain package version',version('langchain'))\n\n!pip install --upgrade langchain\nprint('langchain package version',version('langchain'))","metadata":{"executionCancelledAt":null,"executionTime":7097,"lastExecutedAt":1729935894598,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from importlib.metadata import version\n# #!pip install langchain\n# # Select langchain to 0.1.3\n# try:\n#     assert version('langchain') == '0.1.20'\n# except:\n#     !pip install langchain==0.1.20\n# print('langchain package version',version('langchain'))\n\n!pip install --upgrade langchain\nprint('langchain package version',version('langchain'))","outputsMetadata":{"0":{"height":616,"type":"stream"},"1":{"height":616,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"9128a4c7-81c9-4100-982c-8b2749c31b48","cell_type":"code","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.20)\nCollecting langchain\n  Downloading langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.4.0,>=0.3.12 (from langchain)\n  Downloading langchain_core-0.3.13-py3-none-any.whl.metadata (6.3 kB)\nCollecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.79)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\nCollecting pydantic<3.0.0,>=2.7.4 (from langchain)\n  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.137-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (23.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (4.11.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\nCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\nCollecting pydantic-core==2.23.4 (from pydantic<3.0.0,>=2.7.4->langchain)\n  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.3.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (2.4)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.1)\nDownloading langchain-0.3.4-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.13-py3-none-any.whl (408 kB)\nDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.137-py3-none-any.whl (296 kB)\nDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\nDownloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\nInstalling collected packages: pydantic-core, requests-toolbelt, pydantic, langsmith, langchain-core, langchain-text-splitters, langchain\n\u001b[33m  WARNING: The script langsmith is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-chroma 0.1.3 requires langchain-core<0.3,>=0.1.40, but you have langchain-core 0.3.13 which is incompatible.\nlangchain-huggingface 0.0.3 requires langchain-core<0.3,>=0.1.52, but you have langchain-core 0.3.13 which is incompatible.\ncrewai 0.30.11 requires langchain<0.2.0,>=0.1.10, but you have langchain 0.3.4 which is incompatible.\nembedchain 0.1.110 requires langchain<0.2.0,>=0.1.4, but you have langchain 0.3.4 which is incompatible.\nlangchain-cohere 0.1.5 requires langchain-core<0.3,>=0.1.42, but you have langchain-core 0.3.13 which is incompatible.\nlangchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.3.13 which is incompatible.\nlangchain-openai 0.1.7 requires langchain-core<0.3,>=0.1.46, but you have langchain-core 0.3.13 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.3.4 langchain-core-0.3.13 langchain-text-splitters-0.3.0 langsmith-0.1.137 pydantic-2.9.2 pydantic-core-2.23.4 requests-toolbelt-1.0.0\nlangchain package version 0.3.4\n"}]},{"source":"# # Select langchain-huggingface to 0.0.3\n# try:\n#     assert version('langchain-huggingface') == '0.0.3'\n# except:\n#     !pip install -qU langchain-huggingface==0.0.3\n# print('langchain-huggingface package version',version('langchain-huggingface'))\n\n!pip install -qU langchain-huggingface\nprint('langchain-huggingface version',version('langchain-huggingface'))","metadata":{"executionCancelledAt":null,"executionTime":2793,"lastExecutedAt":1729935897392,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# # Select langchain-huggingface to 0.0.3\n# try:\n#     assert version('langchain-huggingface') == '0.0.3'\n# except:\n#     !pip install -qU langchain-huggingface==0.0.3\n# print('langchain-huggingface package version',version('langchain-huggingface'))\n\n!pip install -qU langchain-huggingface\nprint('langchain-huggingface version',version('langchain-huggingface'))","outputsMetadata":{"0":{"height":122,"type":"stream"},"1":{"height":38,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"0f636bc0-e8d3-457d-85e6-717929034350","cell_type":"code","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"langchain-huggingface version 0.1.0\n"}]},{"source":"# # Select langchain-chroma to 0.1.3\n# try:\n#     assert version('langchain_chroma') == '0.1.3'\n# except:\n#     !pip install -qU langchain_chroma==0.1.3\n# print('langchain_chroma package version',version('langchain_chroma'))\n\n# try:\n#     assert version('langchain_community') == '0.0.38'\n# except:\n#     !pip install -qU langchain_community==0.0.38\n# print('langchain_community package version',version('langchain_community'))\n\n!pip install -qU langchain_chroma\nprint('langchain_chroma version',version('langchain_chroma'))\n!pip install -qU langchain_community\nprint('langchain_community version',version('langchain_community'))","metadata":{"executionCancelledAt":null,"executionTime":7324,"lastExecutedAt":1729935904716,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# # Select langchain-chroma to 0.1.3\n# try:\n#     assert version('langchain_chroma') == '0.1.3'\n# except:\n#     !pip install -qU langchain_chroma==0.1.3\n# print('langchain_chroma package version',version('langchain_chroma'))\n\n# try:\n#     assert version('langchain_community') == '0.0.38'\n# except:\n#     !pip install -qU langchain_community==0.0.38\n# print('langchain_community package version',version('langchain_community'))\n\n!pip install -qU langchain_chroma\nprint('langchain_chroma version',version('langchain_chroma'))\n!pip install -qU langchain_community\nprint('langchain_community version',version('langchain_community'))","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":122,"type":"stream"},"3":{"height":38,"type":"stream"}}},"id":"ac59ccec-47b6-4aee-82f6-fde381ddcfac","cell_type":"code","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"langchain_chroma version 0.1.4\nlangchain_community version 0.3.3\n"}]},{"source":"# try:\n#     assert version('llama-cpp-python') == '0.2.74'\n# except:\n#     !pip install -qU llama-cpp-python==0.2.74\n# print('llama-cpp-python package version',version('llama-cpp-python'))\n\n# !pip install -qU llama-cpp-python\n# print('llama-cpp-python package version',version('llama-cpp-python'))","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1729935904764,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# try:\n#     assert version('llama-cpp-python') == '0.2.74'\n# except:\n#     !pip install -qU llama-cpp-python==0.2.74\n# print('llama-cpp-python package version',version('llama-cpp-python'))\n\n# !pip install -qU llama-cpp-python\n# print('llama-cpp-python package version',version('llama-cpp-python'))","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"id":"02b6191a-53e2-4667-a022-9367b85c7d4d","cell_type":"code","execution_count":4,"outputs":[]},{"source":"# !pip install datamodel_code_generator\n# print('datamodel_code_generator package version',version('datamodel_code_generator'))","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1729935904812,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# !pip install datamodel_code_generator\n# print('datamodel_code_generator package version',version('datamodel_code_generator'))","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"a2237e2d-26d9-49a6-a143-dec4bc9cb853","cell_type":"code","execution_count":5,"outputs":[]},{"source":"# OpenAI \n# Update OpenAI to 1.42.0\ntry:\n    print('openai package version',version('openai'))\n    assert version('openai') == '1.42.0'\nexcept:\n    !pip install openai==1.42.0","metadata":{"executionCancelledAt":null,"executionTime":3178,"lastExecutedAt":1729935907990,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# OpenAI \n# Update OpenAI to 1.42.0\ntry:\n    print('openai package version',version('openai'))\n    assert version('openai') == '1.42.0'\nexcept:\n    !pip install openai==1.42.0","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"64597cca-afc9-49f5-ae1f-c7acb8d22d03","cell_type":"code","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"openai package version 1.30.1\nDefaulting to user installation because normal site-packages is not writeable\nCollecting openai==1.42.0\n  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.42.0) (4.3.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.42.0) (1.7.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.42.0) (0.27.0)\nCollecting jiter<1,>=0.4.0 (from openai==1.42.0)\n  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /home/repl/.local/lib/python3.10/site-packages (from openai==1.42.0) (2.9.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.42.0) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.42.0) (4.66.4)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai==1.42.0) (4.11.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.42.0) (3.7)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.42.0) (1.2.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.42.0) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.42.0) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.42.0) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.42.0) (0.6.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /home/repl/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.42.0) (2.23.4)\nDownloading openai-1.42.0-py3-none-any.whl (362 kB)\nDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\nInstalling collected packages: jiter, openai\n\u001b[33m  WARNING: The script openai is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncrewai 0.30.11 requires langchain<0.2.0,>=0.1.10, but you have langchain 0.3.4 which is incompatible.\nembedchain 0.1.110 requires langchain<0.2.0,>=0.1.4, but you have langchain 0.3.4 which is incompatible.\nlangchain-openai 0.1.7 requires langchain-core<0.3,>=0.1.46, but you have langchain-core 0.3.13 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed jiter-0.6.1 openai-1.42.0\n"}]},{"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport re\nimport os\nos.getcwd()","metadata":{"executionCancelledAt":null,"executionTime":590,"lastExecutedAt":1729935908581,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport re\nimport os\nos.getcwd()"},"id":"9feab293-13ce-4ff7-9fe4-a14e1ecd9109","cell_type":"code","execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":"'/work/files/workspace'"},"metadata":{},"execution_count":7}]},{"source":"# Connect to VectorDB & LLM Agent\n## Connect to VectorDB (Chroma)","metadata":{},"id":"48b5bf0e-be1c-45c8-ab37-ea61a3d80e38","cell_type":"markdown"},{"source":"import chromadb\nfrom langchain_chroma import Chroma\nfrom langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\n\ncollection_name = \"collection_postings\"\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\npersistent_client = chromadb.PersistentClient()\n\nif collection_name in persistent_client.list_collections()[0].name:\n    print(f\"Collection '{collection_name}' exists!\")\n    # Get the existing collection\n    # vector_store = persistent_client.get_collection(collection_name)\n    vector_store = Chroma(client=persistent_client,\n                          collection_name=collection_name,\n                          embedding_function=embeddings)\nelse:\n    print(f\"Collection '{collection_name}' does not exist!\")","metadata":{"executionCancelledAt":null,"executionTime":5401,"lastExecutedAt":1729935913982,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import chromadb\nfrom langchain_chroma import Chroma\nfrom langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\n\ncollection_name = \"collection_postings\"\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\npersistent_client = chromadb.PersistentClient()\n\nif collection_name in persistent_client.list_collections()[0].name:\n    print(f\"Collection '{collection_name}' exists!\")\n    # Get the existing collection\n    # vector_store = persistent_client.get_collection(collection_name)\n    vector_store = Chroma(client=persistent_client,\n                          collection_name=collection_name,\n                          embedding_function=embeddings)\nelse:\n    print(f\"Collection '{collection_name}' does not exist!\")","outputsMetadata":{"0":{"height":38,"type":"stream"},"11":{"height":38,"type":"stream"}}},"id":"3cc5a232-701c-4f88-8fd4-8e922a6a7fc1","cell_type":"code","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"Collection 'collection_postings' exists!\n"}]},{"source":"# # Use the `as_retriever()` function to use it as a retriever in LangChain\n# retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}) #search_kwargs={\"k\": 2, \"fetch_k\": 50}\n\n# retriever","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1729935914032,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# # Use the `as_retriever()` function to use it as a retriever in LangChain\n# retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}) #search_kwargs={\"k\": 2, \"fetch_k\": 50}\n\n# retriever"},"id":"e9ac97ea-17f9-4932-9b15-fa00256a04c5","cell_type":"code","execution_count":9,"outputs":[]},{"source":"## Connect to Agent (Call OpenAI API)","metadata":{},"id":"3d4f8ac2-e8f1-4ce2-a922-e0ecc57dd1b3","cell_type":"markdown"},{"source":"import openai\n\n#initiate the OpenAI client using the API key\nopenai_api_key = os.environ[\"OPENAI_API_KEY\"]\nclient = openai.OpenAI(api_key=openai_api_key)\nclient","metadata":{"executionCancelledAt":null,"executionTime":250,"lastExecutedAt":1729935914282,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import openai\n\n#initiate the OpenAI client using the API key\nopenai_api_key = os.environ[\"OPENAI_API_KEY\"]\nclient = openai.OpenAI(api_key=openai_api_key)\nclient"},"id":"b8ad4829-82f4-4bdd-ba5c-380d61499d38","cell_type":"code","execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":"<openai.OpenAI at 0x7fa994d91750>"},"metadata":{},"execution_count":10}]},{"source":"## Need modification !!!!!","metadata":{},"id":"46857b09-9468-48fa-b074-6ea1cb0fbbab","cell_type":"markdown"},{"source":"# Retrieval and Generation Application","metadata":{},"id":"afaf3ba3-f849-4191-beec-f4c24cf6b2d9","cell_type":"markdown"},{"source":"## Prepare Prompt","metadata":{},"id":"f8118feb-1f1a-49ff-94ab-bf13f2376663","cell_type":"markdown"},{"source":"# extraction_prompt = ''' You are a carear consuler who helps job seekers to find their dream jobs, you give professional advice tailored to the need of your client (i.e., job seeker) according to the following information:\n#     1. Query: Your client's question (enclosed in <query> tag below) that you need to answer\n#     2. Specification: The job post information (enclosed in <specification> tag below) that might best meets your client's requirements\n\n# Upon receiving your aforementioned information, you need to proceed with the following precedures:\n# Step 1. Analyze your client's abilities, including hard and soft skills.\n# Step 2. Analyze the skills needed for the best possible jobs in the job specification\n# Step 3. Summarize your client's strengths that are already sufficient for the job application.\n# Step 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\n# Step 5. Finally, give them advice how to get the jobs mentioned in job specification according the reasoning above.\n\n# To give your client a professional advice, you MUST give the following feedback:\n# 1. Job Position: the best possible job position or title you suggest your client to pursue.\n# 2. Strengths: your client's strengths compared to the job posts\n# 3. Weaknesses: your client's weaknesses compared to the job posts\n# 4. Strateries: the methods you suggest to get the jobs mentioned in job posts \n\n# FINAL note:\n# 1. If you cannot find the relevant informaiton in client's question or job specification for your reasoning, just leave it blank (\"\"). \n# 2. Always give advice according to the information given to you (Question and Job Specification), DO NOT make up answer other than those information!\n\n# Question:\n#     <query>{query}</query>\n# Job Post Information:\n#     <specification>{specification}</specification>\n# Advice:\n# '''","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1729935914332,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# extraction_prompt = ''' You are a carear consuler who helps job seekers to find their dream jobs, you give professional advice tailored to the need of your client (i.e., job seeker) according to the following information:\n#     1. Query: Your client's question (enclosed in <query> tag below) that you need to answer\n#     2. Specification: The job post information (enclosed in <specification> tag below) that might best meets your client's requirements\n\n# Upon receiving your aforementioned information, you need to proceed with the following precedures:\n# Step 1. Analyze your client's abilities, including hard and soft skills.\n# Step 2. Analyze the skills needed for the best possible jobs in the job specification\n# Step 3. Summarize your client's strengths that are already sufficient for the job application.\n# Step 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\n# Step 5. Finally, give them advice how to get the jobs mentioned in job specification according the reasoning above.\n\n# To give your client a professional advice, you MUST give the following feedback:\n# 1. Job Position: the best possible job position or title you suggest your client to pursue.\n# 2. Strengths: your client's strengths compared to the job posts\n# 3. Weaknesses: your client's weaknesses compared to the job posts\n# 4. Strateries: the methods you suggest to get the jobs mentioned in job posts \n\n# FINAL note:\n# 1. If you cannot find the relevant informaiton in client's question or job specification for your reasoning, just leave it blank (\"\"). \n# 2. Always give advice according to the information given to you (Question and Job Specification), DO NOT make up answer other than those information!\n\n# Question:\n#     <query>{query}</query>\n# Job Post Information:\n#     <specification>{specification}</specification>\n# Advice:\n# '''"},"id":"d8f54d12-2ba7-4a80-b189-718f278afab0","cell_type":"code","execution_count":11,"outputs":[]},{"source":"extraction_prompt = ''' You are a carear consuler who helps job seekers to find their dream jobs, you give professional advice tailored to the need of your client (i.e., job seeker) according to the following information:\n    1. Query: Your client's question (enclosed in <query> tag below) that you need to answer\n    2. Specification: The job post information (enclosed in <specification> tag below) that might best meets your client's requirements\n\nUpon receiving your aforementioned information, you need to proceed with the following precedures:\nStep 1. Analyze your client's abilities, including hard and soft skills.\nStep 2. Analyze and summarize the skills needed for the best possible jobs in the job specification\nStep 3. Summarize your client's strengths that are already sufficient for the job application.\nStep 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\nStep 5. Finally, give them advice how to get the jobs mentioned in job specification according the reasoning above. \n\nQuestion:\n    <query>{query}</query>\nJob Post Information:\n    <specification>{specification}</specification>\nAdvice:\n'''","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1729935914380,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"extraction_prompt = ''' You are a carear consuler who helps job seekers to find their dream jobs, you give professional advice tailored to the need of your client (i.e., job seeker) according to the following information:\n    1. Query: Your client's question (enclosed in <query> tag below) that you need to answer\n    2. Specification: The job post information (enclosed in <specification> tag below) that might best meets your client's requirements\n\nUpon receiving your aforementioned information, you need to proceed with the following precedures:\nStep 1. Analyze your client's abilities, including hard and soft skills.\nStep 2. Analyze and summarize the skills needed for the best possible jobs in the job specification\nStep 3. Summarize your client's strengths that are already sufficient for the job application.\nStep 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\nStep 5. Finally, give them advice how to get the jobs mentioned in job specification according the reasoning above. \n\nQuestion:\n    <query>{query}</query>\nJob Post Information:\n    <specification>{specification}</specification>\nAdvice:\n'''"},"id":"a398f6e1-caf7-4899-bb7b-52cc3585f009","cell_type":"code","execution_count":12,"outputs":[]},{"source":"## Preprare Input Query","metadata":{},"id":"6a6dae02-ac78-47c9-8d1c-4bdab15130ee","cell_type":"markdown"},{"source":"query = \"I recently graduated with a Bachelor degree in Computer Science, I use Python and have good grades in machine learning and deep learning. I had various projects that allowed me to apply these skills, from building predictive models to analyzing large datasets. I am now seeking an entry-level data scientist or data analyst role.\"","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1729935956604,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"query = \"I recently graduated with a Bachelor degree in Computer Science, I use Python and have good grades in machine learning and deep learning. I had various projects that allowed me to apply these skills, from building predictive models to analyzing large datasets. I am now seeking an entry-level data scientist or data analyst role.\""},"id":"d6cd857e-728b-43d8-97b7-bf629dbb0a7f","cell_type":"code","execution_count":22,"outputs":[]},{"source":"## Search Results based on Query","metadata":{},"id":"05a61f4d-ea9f-4d5e-9110-3b64499d82be","cell_type":"markdown"},{"source":"# results = retriever.invoke(query) #filter={\"source\": \"news\"}\n# results","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1729935914476,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# results = retriever.invoke(query) #filter={\"source\": \"news\"}\n# results"},"cell_type":"code","id":"1e32707a-7a7a-47e8-83e6-72e446c81789","outputs":[],"execution_count":14},{"source":"results = vector_store.similarity_search_with_score(\n    query , k=5, #filter={\"title\": {\"$in\": keywords}}\n)\ni=0\nspecification = \"\"\nfor res, score in results:\n    print(f\"[{i}][SIM={score:3f}] {res.metadata['title']}\\n---------------------\\n \\\n          {res.page_content} \\n--------------------\\n \\\n           [{res.metadata}]\\n\\n\")\n    specification += ('Title: ' + res.metadata['title'] +'\\n ' + res.page_content)\n    i+=1","metadata":{"executionCancelledAt":null,"executionTime":109,"lastExecutedAt":1729935959750,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"results = vector_store.similarity_search_with_score(\n    query , k=5, #filter={\"title\": {\"$in\": keywords}}\n)\ni=0\nspecification = \"\"\nfor res, score in results:\n    print(f\"[{i}][SIM={score:3f}] {res.metadata['title']}\\n---------------------\\n \\\n          {res.page_content} \\n--------------------\\n \\\n           [{res.metadata}]\\n\\n\")\n    specification += ('Title: ' + res.metadata['title'] +'\\n ' + res.page_content)\n    i+=1","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"94ef0176-fc23-4ba5-8a26-f1fd341dd51d","cell_type":"code","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":"[0][SIM=0.641125] Data Scientist (6+ years) (Fulltime)\n---------------------\n           Job Title: Data ScientistLocation: Bentonville, AR (Onsite)Fulltime  Mode of interview: Video Call Must have skills : AI/ML models using Google Cloud Platform Relevant Experience: 6+ years Education: Bachelor’s Degree or above  Roles & Responsibilities · Proven experience in deploying real-time AI/ML models using Google Cloud Platform.· Strong programming skills in Python and PySpark.· Proficiency with SQL and relational databases, data warehouses, and BigQuery.· Experience in scaling marketing-related AI/ML solutions such as cross/upsell, recommended systems, and category propensity.· Experience in deploying and managing Large scale Machine Learning Models is a plus· Expertise with classical ML algorithm like K-NN, LSH, logistic regression, linear regression, SVM, Random forest and clustering.· Good understanding of ML & DL algorithms and frameworks (Scikit-learn,Spacy, Tensorflow/Keras/ PyTorch)· Experience in deep learning Algorithm s like MLP, CNN, RNN, LSTMs and GANs, Transformers and LLMs.· Excellent programming skills in Python· Expertise in Google Cloud and operationalization of models using MLOPs.· Experience in scheduling jobs for automated training and inference of AI/ML models using airflow or any other workflow orchestration platform.· Proficiency in collecting data from different data sources, data cleaning, preprocessing, and feature engineering.· Understanding of regression, classification, and unsupervised ML algorithms.· Experience in mentoring junior associates in scaling AI/ML models.· Excellent problem-solving and analytical skills.· Strong written and verbal communication skills, with the ability to present and explain complex concepts to both technical and non-technical audiences. \n--------------------\n            [{'job_posting_url': 'https://www.linkedin.com/jobs/view/3902809268/?trk=jobs_biz_prem_srch', 'location': 'Bentonville, AR', 'min_salary': 0, 'title': 'Data Scientist (6+ years) (Fulltime)'}]\n\n\n[1][SIM=0.654880] Data Scientist\n---------------------\n           Primary Location: Manhattan, New York\n\nV-Soft Consulting is currently hiring for a Data Scientist for our premier client in Manhattan, New York.\n\nEducation And Experience »\n\nMasters degree or higher in statistics, computer science, mathematics, economics, engineering, or other technical field.3+ years in a similar role in statistical model risk management.3-5 years in Finance/Insurance.Experience in statistical modeling techniques such as linear regression, logistic regression, survival analysis, GLM, GBM, neural nets, feature engineering and selection, and validation.Experience with comparing methodologies.Strong proficiency in programming using Python, R, and SQL.Experience with statistical modeling using large and complex datasets.\n\nKnowledge, Skills And Abilities »\n\nStrong verbal and written communication skills, listening and teamwork skills.Strong modeling/model validation experience. Predictive, ML, AI models are preferred.Should have a range of experience when it comes to modeling. If a candidate only has experience with one type of financial model, they probably will not get picked up on.Needs to be very strong in Python.Strong communication and written skills.\n\nWhat You’ll Do\n\nJob Responsibilities:\n\nMethodology review and leadership: Reviews model designs, statistical methodology and features. Explores methodology options, stays up-to-date on statistical/ML/AI methods research.Model validation: Performs independent validations of data and models prior to implementation. Validates model performance and business impact post-implementation.Builds challenger models as needed to compare with models being validated.Writes model validation reports summarizing findings from validations.Model inventory and maintenance: Maintains inventory of all statistical models running in production, with input data, use cases, benefits and risks (data, technical, privacy, regulatory, brand)Actively collaborates with the corporate model risk management team (financial, actuarial, etc. models).\n\nInterested?\n\nQualified candidates should send their resumes to ssoni@vsoftconsulting.com\n\nV-Soft Consulting Group is recognized among the top 100 fastest growing staffing companies in North America, V-Soft Consulting Group is headquartered in Louisville, KY with strategic locations in India, Canada and the U.S. V-Soft is known as an agile, innovative technology services company holding several awards and distinctions and has a wide variety of partnerships across diverse technology stacks.\n\nAs a valued V-Soft Consultant, you’re eligible for full benefits (Medical, Dental, Vision), a 401(k) plan, competitive compensation and more. V-Soft is partnered with numerous Fortune 500 companies, exceptionally positioned to advance your career growth.\n\nV-Soft Consulting provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.\n\nFor more information or to view all our open jobs, please visit www.vsoftconsulting.com or call (844) 425-8425.\n\n \n--------------------\n            [{'job_posting_url': 'https://www.linkedin.com/jobs/view/3901947059/?trk=jobs_biz_prem_srch', 'location': 'Manhattan, NY', 'min_salary': 0, 'title': 'Data Scientist'}]\n\n\n[2][SIM=0.666266] Data Scientist II\n---------------------\n           Job Opportunity: Data Scientist II\n\nOur client in the Insurance industry is seeking a talented Data Scientist II to join their Advanced Analytics department.\n\nRole & Responsibilities:\nDevelop and implement data analytics strategies to drive business insightsCreate predictive models and algorithms to optimize decision-making processesAnalyze large datasets to identify trends and patterns for actionable recommendationsCollaborate with cross-functional teams to support data-driven decision-makingPresent findings and recommendations to senior management\nKey Skills:\nStrong experience in data science and analytics - 4 years minimumProficiency in PythonAdvanced knowledge of statistical analysis and data visualization toolsAbility to work with large datasets and databasesExcellent communication and presentation skillsKnowledge in AI/MLWorking experience with Databricks, Azure ML, and Azure CloudWroking experience with health claims dataGenerative AI experience is beneficialMedicaid or Managed Care experience is highly beneficial\nThis is a permanent position offering a competitive salary and benefits package.\n\n \n--------------------\n            [{'job_posting_url': 'https://www.linkedin.com/jobs/view/3903811108/?trk=jobs_biz_prem_srch', 'location': 'Dayton, OH', 'min_salary': 170000.0, 'pay_period': 'YEARLY', 'title': 'Data Scientist II'}]\n\n\n[3][SIM=0.688305] Data Scientist\n---------------------\n           The ideal candidate's favorite words are learning, data, scale, and agility. You will leverage your strong collaboration skills and ability to extract valuable insights from highly complex data sets to ask the right questions and find the right answers. \n Responsibilities\nAnalyze raw data: assessing quality, cleansing, structuring for downstream processing Design accurate and scalable prediction algorithms Collaborate with engineering team to bring analytical prototypes to production Generate actionable insights for business improvements\n\nQualifications\nBachelor's degree or equivalent experience in quantative field (Statistics, Mathematics, Computer Science, Engineering, etc.) At least 1 - 2 years' of experience in quantitative analytics or data modeling Deep understanding of predictive modeling, machine-learning, clustering and classification techniques, and algorithms Fluency in a programming language (Python, C,C++, Java, SQL) Familiarity with Big Data frameworks and visualization tools (Cassandra, Hadoop, Spark, Tableau) \n--------------------\n            [{'job_posting_url': 'https://www.linkedin.com/jobs/view/3886830826/?trk=jobs_biz_prem_srch', 'location': 'California, United States', 'min_salary': 0, 'title': 'Data Scientist'}]\n\n\n[4][SIM=0.704663] Data Analyst II\n---------------------\n           Our client is seeking a Data Engineer to join their team.\n\n Job Title:  Data Engineer\n\n Location:  Alameda, CA\n\n Job Type:  6 Months contract\n\nJob Id: ABOJP00034858\n\nJob Description\n\n Must have data modeling experience.  Data engineering would be even better.  Creating reports in Power BI (advanced) that includes working with a variety of data sources (like Oracle, AWS Redshift, AWS S3, Azure, SharePoint, Excel).  Worked with structured and unstructured data.  Inquisitive and seeks out learning opportunities (what we work on is changing fast, so they need to adapt)  Works well cross functionally (able to speak to business users without using technical jargon) and independently with little oversight \n\n\n List any preferred requirements: \n\n Microsoft Fabric, Microsoft Power Automate  AI (artificial intelligence) ML (machine learning) and gen AI (generative AI) experience or interest a plus  Python/SQL/Spark/DAX  ETL experience  Built solutions with scalability in mind. Creative.  Supply chain experience  3-5 years of experience \n\n\nRequired\n\n Required education level/certifications: \n\n Bachelor’s (Associate’s acceptable with relevant job experience) in Data Analytics, Statistics, Computer Science, or Mathematics major preferred  Certifications in data analytics, data engineering, and/or data architecture would be great  Preferred interview method: phone interview with me then in-person interview \n\n\n Skills:  Data transformation, data modeling, data analysis, demonstrated knowledge with AWS, SQL, Python, MS Excel, Power BI, Fabric, Power Automate\n\nEducation\n\n Four-year college degree (preferred Data Analytics, Statistics, Computer Science, or Mathematics major) or equivalent work experience involving Data Modeling Initiatives and Database Development.  Will also consider an Associate’s with 4+ years of Data Analytics exp.  3-5 years of experience in data modeling, data warehousing, data engineering, and/or data analytics  3+ years of demonstrated experience working with various business units defining and understanding business problems and solutions.  Experience with Supply Chain a plus  Strong understanding of data analytics, structured and unstructured data analysis, predictive modeling techniques, and data visualization as well as a good command of emerging methodologies like artificial intelligence, machine learning, and gen AI  Knows data querying languages (e.g. SQL), scripting languages (e.g. Python), reporting tools such as PowerBI, and advanced Microsoft Excel skills  Experience engaging cross-functional teams to build a strong data infrastructure  A recognized thought leader and innovator in the development of new tools, methodologies, and problem-solving approaches  Broad understanding of the latest data science, analytics, and technology trends/tools and their impact on business strategies and operations  Curiosity-driven, with mindset geared towards continuous learning. Must be comfortable with ambiguity with a proven track record of being a self-starter. \n\n\n Responsibilities: \n\n Unify disparate data sources (different types and systems) into a single source of truth that supports reporting across the teams.  Develop and execute data initiatives on our Digital Transformation roadmap, including development of Power BI reports and dashboards, to enhance data accessibility and productivity for the organization.  Develop solutions for optimal extraction, transformation, and loading of data from various data sources and types.  Benchmark with cross functional teams to understand how technology is utilized to support the business. Collaborate with other teams to understand available and upcoming technology.  Develop technology to support process improvements, data standardization, and automation.  Provide training to department as needed on new tools.  Identify process issues, recommend improvements, and implement process improvements.  Act as a data steward and support the advancement of citizen developers within the department.  Maintain existing Microsoft Fabric and Power Platform applications, including supporting current reporting, metrics, and automation across the team.  Create and maintain the necessary documentation and visuals to ensure clarity in data lineage and firm understanding of developed data solutions. \n\n\n Other responsibilities: \n\n Ability to manage multiple tasks under tight deadlines  Very strong customer focus and relationship skills  Operations/Supply Chain knowledge a plus  Ability to work alone or with little supervision  Willingness to find solutions to problems related to Data analytics  Proficient with Microsoft Suite  Awareness of Project Management methodologies and processes \n\n\nIf this is a role that interests you and you’d like to learn more, click apply now and a recruiter will be in touch with you to discuss this great opportunity. We look forward to speaking with you!\n\nAbout ManpowerGroup, Parent Company of:Manpower, Experis, Talent Solutions, and Jefferson Wells\n\nManpowerGroup® (NYSE: MAN), the leading global workforce solutions company, helps organizations transform in a fast-changing world of work by sourcing, assessing, developing, and managing the talent that enables them to win. We develop innovative solutions for hundreds of thousands of organizations every year, providing them with skilled talent while finding meaningful, sustainable employment for millions of people across a wide range of industries and skills. Our expert family of brands –  Manpower, Experis, Talent Solutions, and Jefferson Wells  – creates substantial value for candidates and clients across more than 75 countries and territories and has done so for over 70 years. We are recognized consistently for our diversity - as a best place to work for Women, Inclusion, Equality and Disability and in 2022 ManpowerGroup was named one of the World's Most Ethical Companies for the 13th year - all confirming our position as the brand of choice for in-demand talent.\n\n \n--------------------\n            [{'job_posting_url': 'https://www.linkedin.com/jobs/view/3898172866/?trk=jobs_biz_prem_srch', 'location': 'Alameda, CA', 'min_salary': 0, 'title': 'Data Analyst II'}]\n\n\n"}]},{"source":"print(specification)","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1729935978204,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(specification)","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"526976e8-2930-4147-a5a4-1380dd76e249","cell_type":"code","execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":"Title: Data Scientist (6+ years) (Fulltime)\n Job Title: Data ScientistLocation: Bentonville, AR (Onsite)Fulltime  Mode of interview: Video Call Must have skills : AI/ML models using Google Cloud Platform Relevant Experience: 6+ years Education: Bachelor’s Degree or above  Roles & Responsibilities · Proven experience in deploying real-time AI/ML models using Google Cloud Platform.· Strong programming skills in Python and PySpark.· Proficiency with SQL and relational databases, data warehouses, and BigQuery.· Experience in scaling marketing-related AI/ML solutions such as cross/upsell, recommended systems, and category propensity.· Experience in deploying and managing Large scale Machine Learning Models is a plus· Expertise with classical ML algorithm like K-NN, LSH, logistic regression, linear regression, SVM, Random forest and clustering.· Good understanding of ML & DL algorithms and frameworks (Scikit-learn,Spacy, Tensorflow/Keras/ PyTorch)· Experience in deep learning Algorithm s like MLP, CNN, RNN, LSTMs and GANs, Transformers and LLMs.· Excellent programming skills in Python· Expertise in Google Cloud and operationalization of models using MLOPs.· Experience in scheduling jobs for automated training and inference of AI/ML models using airflow or any other workflow orchestration platform.· Proficiency in collecting data from different data sources, data cleaning, preprocessing, and feature engineering.· Understanding of regression, classification, and unsupervised ML algorithms.· Experience in mentoring junior associates in scaling AI/ML models.· Excellent problem-solving and analytical skills.· Strong written and verbal communication skills, with the ability to present and explain complex concepts to both technical and non-technical audiences.Title: Data Scientist\n Primary Location: Manhattan, New York\n\nV-Soft Consulting is currently hiring for a Data Scientist for our premier client in Manhattan, New York.\n\nEducation And Experience »\n\nMasters degree or higher in statistics, computer science, mathematics, economics, engineering, or other technical field.3+ years in a similar role in statistical model risk management.3-5 years in Finance/Insurance.Experience in statistical modeling techniques such as linear regression, logistic regression, survival analysis, GLM, GBM, neural nets, feature engineering and selection, and validation.Experience with comparing methodologies.Strong proficiency in programming using Python, R, and SQL.Experience with statistical modeling using large and complex datasets.\n\nKnowledge, Skills And Abilities »\n\nStrong verbal and written communication skills, listening and teamwork skills.Strong modeling/model validation experience. Predictive, ML, AI models are preferred.Should have a range of experience when it comes to modeling. If a candidate only has experience with one type of financial model, they probably will not get picked up on.Needs to be very strong in Python.Strong communication and written skills.\n\nWhat You’ll Do\n\nJob Responsibilities:\n\nMethodology review and leadership: Reviews model designs, statistical methodology and features. Explores methodology options, stays up-to-date on statistical/ML/AI methods research.Model validation: Performs independent validations of data and models prior to implementation. Validates model performance and business impact post-implementation.Builds challenger models as needed to compare with models being validated.Writes model validation reports summarizing findings from validations.Model inventory and maintenance: Maintains inventory of all statistical models running in production, with input data, use cases, benefits and risks (data, technical, privacy, regulatory, brand)Actively collaborates with the corporate model risk management team (financial, actuarial, etc. models).\n\nInterested?\n\nQualified candidates should send their resumes to ssoni@vsoftconsulting.com\n\nV-Soft Consulting Group is recognized among the top 100 fastest growing staffing companies in North America, V-Soft Consulting Group is headquartered in Louisville, KY with strategic locations in India, Canada and the U.S. V-Soft is known as an agile, innovative technology services company holding several awards and distinctions and has a wide variety of partnerships across diverse technology stacks.\n\nAs a valued V-Soft Consultant, you’re eligible for full benefits (Medical, Dental, Vision), a 401(k) plan, competitive compensation and more. V-Soft is partnered with numerous Fortune 500 companies, exceptionally positioned to advance your career growth.\n\nV-Soft Consulting provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.\n\nFor more information or to view all our open jobs, please visit www.vsoftconsulting.com or call (844) 425-8425.\n\nTitle: Data Scientist II\n Job Opportunity: Data Scientist II\n\nOur client in the Insurance industry is seeking a talented Data Scientist II to join their Advanced Analytics department.\n\nRole & Responsibilities:\nDevelop and implement data analytics strategies to drive business insightsCreate predictive models and algorithms to optimize decision-making processesAnalyze large datasets to identify trends and patterns for actionable recommendationsCollaborate with cross-functional teams to support data-driven decision-makingPresent findings and recommendations to senior management\nKey Skills:\nStrong experience in data science and analytics - 4 years minimumProficiency in PythonAdvanced knowledge of statistical analysis and data visualization toolsAbility to work with large datasets and databasesExcellent communication and presentation skillsKnowledge in AI/MLWorking experience with Databricks, Azure ML, and Azure CloudWroking experience with health claims dataGenerative AI experience is beneficialMedicaid or Managed Care experience is highly beneficial\nThis is a permanent position offering a competitive salary and benefits package.\n\nTitle: Data Scientist\n The ideal candidate's favorite words are learning, data, scale, and agility. You will leverage your strong collaboration skills and ability to extract valuable insights from highly complex data sets to ask the right questions and find the right answers. \n Responsibilities\nAnalyze raw data: assessing quality, cleansing, structuring for downstream processing Design accurate and scalable prediction algorithms Collaborate with engineering team to bring analytical prototypes to production Generate actionable insights for business improvements\n\nQualifications\nBachelor's degree or equivalent experience in quantative field (Statistics, Mathematics, Computer Science, Engineering, etc.) At least 1 - 2 years' of experience in quantitative analytics or data modeling Deep understanding of predictive modeling, machine-learning, clustering and classification techniques, and algorithms Fluency in a programming language (Python, C,C++, Java, SQL) Familiarity with Big Data frameworks and visualization tools (Cassandra, Hadoop, Spark, Tableau)Title: Data Analyst II\n Our client is seeking a Data Engineer to join their team.\n\n Job Title:  Data Engineer\n\n Location:  Alameda, CA\n\n Job Type:  6 Months contract\n\nJob Id: ABOJP00034858\n\nJob Description\n\n Must have data modeling experience.  Data engineering would be even better.  Creating reports in Power BI (advanced) that includes working with a variety of data sources (like Oracle, AWS Redshift, AWS S3, Azure, SharePoint, Excel).  Worked with structured and unstructured data.  Inquisitive and seeks out learning opportunities (what we work on is changing fast, so they need to adapt)  Works well cross functionally (able to speak to business users without using technical jargon) and independently with little oversight \n\n\n List any preferred requirements: \n\n Microsoft Fabric, Microsoft Power Automate  AI (artificial intelligence) ML (machine learning) and gen AI (generative AI) experience or interest a plus  Python/SQL/Spark/DAX  ETL experience  Built solutions with scalability in mind. Creative.  Supply chain experience  3-5 years of experience \n\n\nRequired\n\n Required education level/certifications: \n\n Bachelor’s (Associate’s acceptable with relevant job experience) in Data Analytics, Statistics, Computer Science, or Mathematics major preferred  Certifications in data analytics, data engineering, and/or data architecture would be great  Preferred interview method: phone interview with me then in-person interview \n\n\n Skills:  Data transformation, data modeling, data analysis, demonstrated knowledge with AWS, SQL, Python, MS Excel, Power BI, Fabric, Power Automate\n\nEducation\n\n Four-year college degree (preferred Data Analytics, Statistics, Computer Science, or Mathematics major) or equivalent work experience involving Data Modeling Initiatives and Database Development.  Will also consider an Associate’s with 4+ years of Data Analytics exp.  3-5 years of experience in data modeling, data warehousing, data engineering, and/or data analytics  3+ years of demonstrated experience working with various business units defining and understanding business problems and solutions.  Experience with Supply Chain a plus  Strong understanding of data analytics, structured and unstructured data analysis, predictive modeling techniques, and data visualization as well as a good command of emerging methodologies like artificial intelligence, machine learning, and gen AI  Knows data querying languages (e.g. SQL), scripting languages (e.g. Python), reporting tools such as PowerBI, and advanced Microsoft Excel skills  Experience engaging cross-functional teams to build a strong data infrastructure  A recognized thought leader and innovator in the development of new tools, methodologies, and problem-solving approaches  Broad understanding of the latest data science, analytics, and technology trends/tools and their impact on business strategies and operations  Curiosity-driven, with mindset geared towards continuous learning. Must be comfortable with ambiguity with a proven track record of being a self-starter. \n\n\n Responsibilities: \n\n Unify disparate data sources (different types and systems) into a single source of truth that supports reporting across the teams.  Develop and execute data initiatives on our Digital Transformation roadmap, including development of Power BI reports and dashboards, to enhance data accessibility and productivity for the organization.  Develop solutions for optimal extraction, transformation, and loading of data from various data sources and types.  Benchmark with cross functional teams to understand how technology is utilized to support the business. Collaborate with other teams to understand available and upcoming technology.  Develop technology to support process improvements, data standardization, and automation.  Provide training to department as needed on new tools.  Identify process issues, recommend improvements, and implement process improvements.  Act as a data steward and support the advancement of citizen developers within the department.  Maintain existing Microsoft Fabric and Power Platform applications, including supporting current reporting, metrics, and automation across the team.  Create and maintain the necessary documentation and visuals to ensure clarity in data lineage and firm understanding of developed data solutions. \n\n\n Other responsibilities: \n\n Ability to manage multiple tasks under tight deadlines  Very strong customer focus and relationship skills  Operations/Supply Chain knowledge a plus  Ability to work alone or with little supervision  Willingness to find solutions to problems related to Data analytics  Proficient with Microsoft Suite  Awareness of Project Management methodologies and processes \n\n\nIf this is a role that interests you and you’d like to learn more, click apply now and a recruiter will be in touch with you to discuss this great opportunity. We look forward to speaking with you!\n\nAbout ManpowerGroup, Parent Company of:Manpower, Experis, Talent Solutions, and Jefferson Wells\n\nManpowerGroup® (NYSE: MAN), the leading global workforce solutions company, helps organizations transform in a fast-changing world of work by sourcing, assessing, developing, and managing the talent that enables them to win. We develop innovative solutions for hundreds of thousands of organizations every year, providing them with skilled talent while finding meaningful, sustainable employment for millions of people across a wide range of industries and skills. Our expert family of brands –  Manpower, Experis, Talent Solutions, and Jefferson Wells  – creates substantial value for candidates and clients across more than 75 countries and territories and has done so for over 70 years. We are recognized consistently for our diversity - as a best place to work for Women, Inclusion, Equality and Disability and in 2022 ManpowerGroup was named one of the World's Most Ethical Companies for the 13th year - all confirming our position as the brand of choice for in-demand talent.\n\n\n"}]},{"source":"## Get Final Response","metadata":{},"id":"14489047-ec44-4f3a-8d77-1ecf13b7a268","cell_type":"markdown"},{"source":"prompt_all = extraction_prompt.format(query=query, specification=specification)\nprint(prompt_all)","metadata":{"executionCancelledAt":null,"executionTime":13,"lastExecutedAt":1729935986235,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"prompt_all = extraction_prompt.format(query=query, specification=specification)\nprint(prompt_all)","outputsMetadata":{"0":{"height":458,"type":"stream"}}},"id":"e23b3f1f-6a18-4433-8164-608250f8e168","cell_type":"code","execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":" You are a carear consuler who helps job seekers to find their dream jobs, you give professional advice tailored to the need of your client (i.e., job seeker) according to the following information:\n    1. Query: Your client's question (enclosed in <query> tag below) that you need to answer\n\n\nUpon receiving your aforementioned information, you need to proceed with the following precedures:\nStep 1. Analyze your client's abilities, including hard and soft skills.\nStep 2. Analyze and summarize the skills needed for the best possible jobs\nStep 3. Summarize your client's strengths that are already sufficient for the job application.\nStep 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\nStep 5. Finally, give them advice how to get the jobs.\n\nQuestion:\n    <query>I recently graduated with a Bachelor degree in Computer Science, I use Python and have good grades in machine learning and deep learning. I had various projects that allowed me to apply these skills, from building predictive models to analyzing large datasets. I am now seeking an entry-level data scientist or data analyst role.</query>\n\nAdvice:\n\n"}]},{"source":"import tiktoken\n\n# Define a function to count tokens for a given prompt and model\ndef count_tokens(text, model=\"gpt-3.5-turbo-instruct\"):\n    encoding = tiktoken.encoding_for_model(model)\n    return len(encoding.encode(text))\n\n# Count the number of tokens in the prompt\nprompt_tokens = count_tokens(prompt_all);print(f\"total prompt tokens = {prompt_tokens}\")\n\n# Token limit for gpt-3.5-turbo-instruct\ntoken_limit = 4097\n\n# Ensure the total tokens (prompt + response) is within the limit\n# Assume you want the model to generate a maximum of 1000 tokens in the response\nresponse_max_tokens = 1000\nif prompt_tokens + response_max_tokens > token_limit:\n    print('total token size exceeds limit, start trimming!')\n    # Calculate the allowable prompt length\n    max_prompt_tokens = token_limit - response_max_tokens\n\n    # Trim the prompt to fit within the token limit\n    trimmed_prompt = prompt_all[:max_prompt_tokens]\n\n    # Notify user about trimming\n    print(f\"Prompt trimmed from {prompt_tokens} to {max_prompt_tokens} tokens.\")\n    print(\"final prompt_all:\\n\",prompt_all)\n\n    # Update the prompt with the trimmed version\n    prompt_all = trimmed_prompt\nelse:\n    print('total token size doesn\\'t  exceeds limit, good job!')\n\n","metadata":{"executionCancelledAt":null,"executionTime":13,"lastExecutedAt":1729935997918,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import tiktoken\n\n# Define a function to count tokens for a given prompt and model\ndef count_tokens(text, model=\"gpt-3.5-turbo-instruct\"):\n    encoding = tiktoken.encoding_for_model(model)\n    return len(encoding.encode(text))\n\n# Count the number of tokens in the prompt\nprompt_tokens = count_tokens(prompt_all);print(f\"total prompt tokens = {prompt_tokens}\")\n\n# Token limit for gpt-3.5-turbo-instruct\ntoken_limit = 4097\n\n# Ensure the total tokens (prompt + response) is within the limit\n# Assume you want the model to generate a maximum of 1000 tokens in the response\nresponse_max_tokens = 1000\nif prompt_tokens + response_max_tokens > token_limit:\n    print('total token size exceeds limit, start trimming!')\n    # Calculate the allowable prompt length\n    max_prompt_tokens = token_limit - response_max_tokens\n\n    # Trim the prompt to fit within the token limit\n    trimmed_prompt = prompt_all[:max_prompt_tokens]\n\n    # Notify user about trimming\n    print(f\"Prompt trimmed from {prompt_tokens} to {max_prompt_tokens} tokens.\")\n    print(\"final prompt_all:\\n\",prompt_all)\n\n    # Update the prompt with the trimmed version\n    prompt_all = trimmed_prompt\nelse:\n    print('total token size doesn\\'t  exceeds limit, good job!')\n\n","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"id":"970a2dea-77d9-4364-b5f0-e9556f9e616a","cell_type":"code","execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":"total prompt tokens = 245\ntotal token size doesn't  exceeds limit, good job!\n"}]},{"source":"response = client.completions.create(model=\"gpt-3.5-turbo-instruct\",  \n                                     prompt=prompt_all,\n                                     max_tokens=response_max_tokens) \nprint(response.choices[0].text)","metadata":{"executionCancelledAt":null,"executionTime":5187,"lastExecutedAt":1729936007254,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"response = client.completions.create(model=\"gpt-3.5-turbo-instruct\",  \n                                     prompt=prompt_all,\n                                     max_tokens=response_max_tokens) \nprint(response.choices[0].text)","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"2e7980fe-46db-48c4-ae60-b12a5da8bdbf","cell_type":"code","execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":"Step 1. Analyze your client's abilities, including hard and soft skills.\n\nYour client has a Bachelor degree in Computer Science with a focus on machine learning and deep learning. They also have experience working on projects that involve building predictive models and analyzing large datasets. Based on this, it can be inferred that your client has strong analytical and problem-solving skills, proficiency in programming languages such as Python, and the ability to handle and make sense of large amounts of data.\n\nStep 2. Analyze and summarize the skills needed for the best possible jobs\n\nFor entry-level data scientist or data analyst roles, the following skills are typically required:\n1. Proficiency in programming languages such as Python, R, and SQL\n2. Knowledge of data analysis and machine learning techniques\n3. Understanding of statistics and probability\n4. Ability to work with large datasets\n5. Problem-solving and critical thinking skills\n6. Attention to detail and accuracy\n7. Effective communication and collaboration skills\n8. Familiarity with data visualization tools\n9. Ability to learn and adapt to new technologies and techniques\n10. Time management and organization skills.\n\nStep 3. Summarize your client's strengths that are already sufficient for the job application.\n\nBased on their abilities and the skills needed for data scientist or data analyst roles, your client's strengths include strong analytical and problem-solving skills, proficiency in Python, ability to work with large datasets, and experience with data analysis and machine learning techniques.\n\nStep 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\n\nIn order to meet the job requirements and become a stronger candidate, your client may need to improve in the following areas:\n1. Familiarity with other programming languages such as R and SQL\n2. Understanding of statistics and probability concepts\n3. Communication and collaboration skills\n4. Knowledge of data visualization tools.\n\nStep 5. Finally, give them advice how to get the jobs.\n\nTo improve their chances of getting an entry-level data scientist or data analyst role, here are a few suggestions:\n1. Brush up on or gain proficiency in programming languages such as R and SQL, as these are commonly used in data science and analysis.\n2. Expand their knowledge of statistics and probability, as these are important for understanding and interpreting data.\n3. Practice communicating and collaborating effectively, as these skills are highly valued in a team-based work environment.\n4. Familiarize themselves with data visualization tools, as this can greatly enhance their ability to present and communicate data insights.\n5. Network and seek out opportunities for internships, volunteer work, or projects that allow them to gain hands-on experience and further develop their skills.\n6. Continuously learn and stay up-to-date on the latest technologies and techniques in data science and analysis. \n"}]},{"source":"# What If: Generation without Application","metadata":{},"cell_type":"markdown","id":"896a2613-5202-479a-802f-2d5dcd493e94"},{"source":"extraction_prompt = ''' You are a carear consuler who helps job seekers to find their dream jobs, you give professional advice tailored to the need of your client (i.e., job seeker) according to the following information:\n    1. Query: Your client's question (enclosed in <query> tag below) that you need to answer\n\n\nUpon receiving your aforementioned information, you need to proceed with the following precedures:\nStep 1. Analyze your client's abilities, including hard and soft skills.\nStep 2. Analyze and summarize the skills needed for the best possible jobs\nStep 3. Summarize your client's strengths that are already sufficient for the job application.\nStep 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\nStep 5. Finally, give them advice how to get the jobs.\n\nQuestion:\n    <query>{query}</query>\n\nAdvice:\n'''\n\nprompt_all = extraction_prompt.format(query=query)","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1729936036272,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"extraction_prompt = ''' You are a carear consuler who helps job seekers to find their dream jobs, you give professional advice tailored to the need of your client (i.e., job seeker) according to the following information:\n    1. Query: Your client's question (enclosed in <query> tag below) that you need to answer\n\n\nUpon receiving your aforementioned information, you need to proceed with the following precedures:\nStep 1. Analyze your client's abilities, including hard and soft skills.\nStep 2. Analyze and summarize the skills needed for the best possible jobs\nStep 3. Summarize your client's strengths that are already sufficient for the job application.\nStep 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\nStep 5. Finally, give them advice how to get the jobs.\n\nQuestion:\n    <query>{query}</query>\n\nAdvice:\n'''\n\nprompt_all = extraction_prompt.format(query=query)","outputsMetadata":{"0":{"height":563,"type":"stream"}}},"cell_type":"code","id":"90e5f89d-998e-4d94-95af-63db4312c944","outputs":[],"execution_count":28},{"source":"response = client.completions.create(model=\"gpt-3.5-turbo-instruct\",  \n                                     prompt=prompt_all,\n                                     max_tokens=response_max_tokens) \nprint(response.choices[0].text)","metadata":{"executionCancelledAt":null,"executionTime":2459,"lastExecutedAt":1729936047520,"lastExecutedByKernel":"a89a7bc1-1549-402a-bc95-5aac60bc1893","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"response = client.completions.create(model=\"gpt-3.5-turbo-instruct\",  \n                                     prompt=prompt_all,\n                                     max_tokens=response_max_tokens) \nprint(response.choices[0].text)","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"bfb7229b-7156-40d7-bd5e-ae5972553502","outputs":[{"output_type":"stream","name":"stdout","text":"    Step 1. Your client's hard skills include a Bachelor degree in Computer Science and proficiency in Python, machine learning and deep learning, as well as experience in building predictive models and analyzing large datasets.\n\n    Step 2. The skills needed for an entry-level data scientist or data analyst role include knowledge of programming languages such as Python, R or SQL, knowledge of statistics and machine learning, and experience in data analysis and data visualization.\n\n    Step 3. Your client's strengths that are already sufficient for the job application include their Bachelor degree in Computer Science and their proficiency in Python, machine learning and deep learning.\n\n    Step 4. Your client's weaknesses that they need to improve on in order to meet the job requirements include their lack of experience in other programming languages such as R or SQL, and their need to gain more experience in data analysis and data visualization.\n\n    Step 5. My advice for your client would be to focus on gaining more experience in data analysis and data visualization, as well as learning other programming languages such as R or SQL. They could do this through online courses, internships, or personal projects. It would also be beneficial for them to network and attend job fairs to make connections and increase their chances of finding an entry-level data scientist or data analyst role.\n"}],"execution_count":29}],"metadata":{"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}