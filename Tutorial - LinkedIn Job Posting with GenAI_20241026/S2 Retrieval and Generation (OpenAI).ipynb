{
  "cells": [
    {
      "source": [
        "# Retrieval and Generation\n",
        "\n",
        "**Vector Database (Vector DB)**\n",
        "Resources\n",
        "- [How-to guides](https://python.langchain.com/v0.2/docs/how_to/#vector-stores)\n",
        "  - [Vectorstores](https://python.langchain.com/v0.2/docs/integrations/vectorstores/): A vector store that stores embedded data and performs similarity search.\n",
        "    1. [Elasticsearch](https://python.langchain.com/v0.2/docs/integrations/vectorstores/elasticsearch/)\n",
        "    2. [Milvus](https://python.langchain.com/v0.2/docs/integrations/vectorstores/milvus/)\n",
        "    3. [Chroma](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)"
      ],
      "metadata": {
        "id": "3134fe35-1ec3-4179-8f02-1e49de949f5b"
      },
      "id": "3134fe35-1ec3-4179-8f02-1e49de949f5b",
      "cell_type": "markdown"
    },
    {
      "source": [
        "# Preface\n",
        "## Environment Setup\n",
        "\n",
        "Sources  \n",
        "- [langchain-chroma](https://pypi.org/project/langchain-chroma/)"
      ],
      "metadata": {
        "id": "2b4d9104-6cbd-475d-ab1d-f2faf8f636bc"
      },
      "id": "2b4d9104-6cbd-475d-ab1d-f2faf8f636bc",
      "cell_type": "markdown"
    },
    {
      "source": [
        "from importlib.metadata import version\n",
        "# #!pip install langchain\n",
        "# # Select langchain to 0.1.3\n",
        "# try:\n",
        "#     assert version('langchain') == '0.1.20'\n",
        "# except:\n",
        "#     !pip install langchain==0.1.20\n",
        "# print('langchain package version',version('langchain'))\n",
        "\n",
        "!pip install --upgrade langchain\n",
        "print('langchain package version',version('langchain'))"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 7097,
        "lastExecutedAt": 1729935894598,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "from importlib.metadata import version\n# #!pip install langchain\n# # Select langchain to 0.1.3\n# try:\n#     assert version('langchain') == '0.1.20'\n# except:\n#     !pip install langchain==0.1.20\n# print('langchain package version',version('langchain'))\n\n!pip install --upgrade langchain\nprint('langchain package version',version('langchain'))",
        "outputsMetadata": {
          "0": {
            "height": 616,
            "type": "stream"
          },
          "1": {
            "height": 616,
            "type": "stream"
          }
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "id": "9128a4c7-81c9-4100-982c-8b2749c31b48",
        "outputId": "85a55baf-e5dc-4613-b9a8-33e96bb0ceda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9128a4c7-81c9-4100-982c-8b2749c31b48",
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.13)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (3.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "langchain package version 0.3.4\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# # Select langchain-huggingface to 0.0.3\n",
        "# try:\n",
        "#     assert version('langchain-huggingface') == '0.0.3'\n",
        "# except:\n",
        "#     !pip install -qU langchain-huggingface==0.0.3\n",
        "# print('langchain-huggingface package version',version('langchain-huggingface'))\n",
        "\n",
        "!pip install -qU langchain-huggingface\n",
        "print('langchain-huggingface version',version('langchain-huggingface'))"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 2793,
        "lastExecutedAt": 1729935897392,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "# # Select langchain-huggingface to 0.0.3\n# try:\n#     assert version('langchain-huggingface') == '0.0.3'\n# except:\n#     !pip install -qU langchain-huggingface==0.0.3\n# print('langchain-huggingface package version',version('langchain-huggingface'))\n\n!pip install -qU langchain-huggingface\nprint('langchain-huggingface version',version('langchain-huggingface'))",
        "outputsMetadata": {
          "0": {
            "height": 122,
            "type": "stream"
          },
          "1": {
            "height": 38,
            "type": "stream"
          }
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "id": "0f636bc0-e8d3-457d-85e6-717929034350",
        "outputId": "7d94feee-552d-46c4-df8e-8751dd38052d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0f636bc0-e8d3-457d-85e6-717929034350",
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain-huggingface version 0.1.0\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# # Select langchain-chroma to 0.1.3\n",
        "# try:\n",
        "#     assert version('langchain_chroma') == '0.1.3'\n",
        "# except:\n",
        "#     !pip install -qU langchain_chroma==0.1.3\n",
        "# print('langchain_chroma package version',version('langchain_chroma'))\n",
        "\n",
        "# try:\n",
        "#     assert version('langchain_community') == '0.0.38'\n",
        "# except:\n",
        "#     !pip install -qU langchain_community==0.0.38\n",
        "# print('langchain_community package version',version('langchain_community'))\n",
        "\n",
        "!pip install -qU langchain_chroma\n",
        "print('langchain_chroma version',version('langchain_chroma'))\n",
        "!pip install -qU langchain_community\n",
        "print('langchain_community version',version('langchain_community'))"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 7324,
        "lastExecutedAt": 1729935904716,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "# # Select langchain-chroma to 0.1.3\n# try:\n#     assert version('langchain_chroma') == '0.1.3'\n# except:\n#     !pip install -qU langchain_chroma==0.1.3\n# print('langchain_chroma package version',version('langchain_chroma'))\n\n# try:\n#     assert version('langchain_community') == '0.0.38'\n# except:\n#     !pip install -qU langchain_community==0.0.38\n# print('langchain_community package version',version('langchain_community'))\n\n!pip install -qU langchain_chroma\nprint('langchain_chroma version',version('langchain_chroma'))\n!pip install -qU langchain_community\nprint('langchain_community version',version('langchain_community'))",
        "outputsMetadata": {
          "0": {
            "height": 38,
            "type": "stream"
          },
          "1": {
            "height": 38,
            "type": "stream"
          },
          "2": {
            "height": 122,
            "type": "stream"
          },
          "3": {
            "height": 38,
            "type": "stream"
          }
        },
        "id": "ac59ccec-47b6-4aee-82f6-fde381ddcfac",
        "outputId": "5ba81708-b4de-4083-84dc-ad584f25b2bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ac59ccec-47b6-4aee-82f6-fde381ddcfac",
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain_chroma version 0.1.4\n",
            "langchain_community version 0.3.3\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# try:\n",
        "#     assert version('llama-cpp-python') == '0.2.74'\n",
        "# except:\n",
        "#     !pip install -qU llama-cpp-python==0.2.74\n",
        "# print('llama-cpp-python package version',version('llama-cpp-python'))\n",
        "\n",
        "# !pip install -qU llama-cpp-python\n",
        "# print('llama-cpp-python package version',version('llama-cpp-python'))"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1729935904764,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "# try:\n#     assert version('llama-cpp-python') == '0.2.74'\n# except:\n#     !pip install -qU llama-cpp-python==0.2.74\n# print('llama-cpp-python package version',version('llama-cpp-python'))\n\n# !pip install -qU llama-cpp-python\n# print('llama-cpp-python package version',version('llama-cpp-python'))",
        "outputsMetadata": {
          "0": {
            "height": 38,
            "type": "stream"
          }
        },
        "id": "02b6191a-53e2-4667-a022-9367b85c7d4d"
      },
      "id": "02b6191a-53e2-4667-a022-9367b85c7d4d",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# !pip install datamodel_code_generator\n",
        "# print('datamodel_code_generator package version',version('datamodel_code_generator'))"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1729935904812,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "# !pip install datamodel_code_generator\n# print('datamodel_code_generator package version',version('datamodel_code_generator'))",
        "outputsMetadata": {
          "0": {
            "height": 616,
            "type": "stream"
          }
        },
        "id": "a2237e2d-26d9-49a6-a143-dec4bc9cb853"
      },
      "id": "a2237e2d-26d9-49a6-a143-dec4bc9cb853",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# OpenAI\n",
        "# Update OpenAI to 1.42.0\n",
        "try:\n",
        "    print('openai package version',version('openai'))\n",
        "    assert version('openai') == '1.42.0'\n",
        "except:\n",
        "    !pip install openai==1.42.0"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 3178,
        "lastExecutedAt": 1729935907990,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "# OpenAI \n# Update OpenAI to 1.42.0\ntry:\n    print('openai package version',version('openai'))\n    assert version('openai') == '1.42.0'\nexcept:\n    !pip install openai==1.42.0",
        "outputsMetadata": {
          "0": {
            "height": 616,
            "type": "stream"
          }
        },
        "id": "64597cca-afc9-49f5-ae1f-c7acb8d22d03",
        "outputId": "1ab1f749-0a5d-4a8f-bfbd-e1c76f833609",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "64597cca-afc9-49f5-ae1f-c7acb8d22d03",
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openai package version 1.42.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "g-eUMIbb0r8C",
        "outputId": "b244d009-b1af-44ff-a9f2-6e96b9df5ac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "g-eUMIbb0r8C",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 590,
        "lastExecutedAt": 1729935908581,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport re\nimport os\nos.getcwd()",
        "id": "9feab293-13ce-4ff7-9fe4-a14e1ecd9109",
        "outputId": "3e6b88d4-ff3b-4b90-bbee-1949de832ac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "9feab293-13ce-4ff7-9fe4-a14e1ecd9109",
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "metadata": {
        "id": "FMC8ct_7xuJn",
        "outputId": "b8632003-d593-4042-b5ea-8bf48c3ba22c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FMC8ct_7xuJn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chroma\tsample_data\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Connect to VectorDB & LLM Agent\n",
        "## Connect to VectorDB (Chroma)"
      ],
      "metadata": {
        "id": "48b5bf0e-be1c-45c8-ab37-ea61a3d80e38"
      },
      "id": "48b5bf0e-be1c-45c8-ab37-ea61a3d80e38",
      "cell_type": "markdown"
    },
    {
      "source": [
        "import chromadb\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\n",
        "\n",
        "collection_name = \"collection_postings\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "persistent_client = chromadb.PersistentClient()\n",
        "print(persistent_client.list_collections())\n",
        "\n",
        "vector_store = Chroma(client=persistent_client,\n",
        "                      collection_name=collection_name,\n",
        "                      embedding_function=embeddings)\n",
        "\n",
        "# try:\n",
        "#   if collection_name in persistent_client.list_collections()[0].name:\n",
        "#       print(f\"Collection '{collection_name}' exists!\")\n",
        "#       # Get the existing collection\n",
        "#       # vector_store = persistent_client.get_collection(collection_name)\n",
        "#       vector_store = Chroma(client=persistent_client,\n",
        "#                             collection_name=collection_name,\n",
        "#                             embedding_function=embeddings)\n",
        "# except:\n",
        "#     print(f\"Collection '{collection_name}' does not exist!\")"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 5401,
        "lastExecutedAt": 1729935913982,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "import chromadb\nfrom langchain_chroma import Chroma\nfrom langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\n\ncollection_name = \"collection_postings\"\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\npersistent_client = chromadb.PersistentClient()\n\nif collection_name in persistent_client.list_collections()[0].name:\n    print(f\"Collection '{collection_name}' exists!\")\n    # Get the existing collection\n    # vector_store = persistent_client.get_collection(collection_name)\n    vector_store = Chroma(client=persistent_client,\n                          collection_name=collection_name,\n                          embedding_function=embeddings)\nelse:\n    print(f\"Collection '{collection_name}' does not exist!\")",
        "outputsMetadata": {
          "0": {
            "height": 38,
            "type": "stream"
          },
          "11": {
            "height": 38,
            "type": "stream"
          }
        },
        "id": "3cc5a232-701c-4f88-8fd4-8e922a6a7fc1",
        "outputId": "ee97066d-0481-4bdc-dd85-ba2faefa9013",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3cc5a232-701c-4f88-8fd4-8e922a6a7fc1",
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Collection(id=9799cf17-fa1a-462c-818a-b8625701e935, name=collection_postings)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: how can I see the data in vector_store?\n",
        "\n",
        "# Get all the documents in the vector store\n",
        "documents = vector_store.get()\n",
        "\n",
        "# Print the documents\n",
        "print(documents)\n",
        "\n",
        "# Alternatively, you can get the embeddings and ids\n",
        "embeddings = vector_store.get()['embeddings']\n",
        "ids = vector_store.get()['ids']\n",
        "\n",
        "# Print the embeddings\n",
        "print(embeddings)\n",
        "\n",
        "# Print the ids\n",
        "ids"
      ],
      "metadata": {
        "id": "E1XloNBw1qBo",
        "outputId": "ffafede6-cb61-4a62-c1ba-7edd9405fab8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "E1XloNBw1qBo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ids': [], 'embeddings': None, 'documents': [], 'uris': None, 'data': None, 'metadatas': [], 'included': [<IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "m0QfevnP1QM-",
        "outputId": "86733568-2dc6-4bca-eb9d-36375f797aa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "m0QfevnP1QM-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Collection(id=9799cf17-fa1a-462c-818a-b8625701e935, name=collection_postings)]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "source": [
        "# # Use the `as_retriever()` function to use it as a retriever in LangChain\n",
        "# retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}) #search_kwargs={\"k\": 2, \"fetch_k\": 50}\n",
        "\n",
        "# retriever"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 50,
        "lastExecutedAt": 1729935914032,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "# # Use the `as_retriever()` function to use it as a retriever in LangChain\n# retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}) #search_kwargs={\"k\": 2, \"fetch_k\": 50}\n\n# retriever",
        "id": "e9ac97ea-17f9-4932-9b15-fa00256a04c5"
      },
      "id": "e9ac97ea-17f9-4932-9b15-fa00256a04c5",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Connect to Agent (Call OpenAI API)"
      ],
      "metadata": {
        "id": "3d4f8ac2-e8f1-4ce2-a922-e0ecc57dd1b3"
      },
      "id": "3d4f8ac2-e8f1-4ce2-a922-e0ecc57dd1b3",
      "cell_type": "markdown"
    },
    {
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "#initiate the OpenAI client using the API key\n",
        "# openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "client = openai.OpenAI(api_key=openai_api_key)\n",
        "client"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 250,
        "lastExecutedAt": 1729935914282,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "import openai\n\n#initiate the OpenAI client using the API key\nopenai_api_key = os.environ[\"OPENAI_API_KEY\"]\nclient = openai.OpenAI(api_key=openai_api_key)\nclient",
        "id": "b8ad4829-82f4-4bdd-ba5c-380d61499d38",
        "outputId": "5b3dff7c-b94a-4fb3-e801-952098ee08d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "b8ad4829-82f4-4bdd-ba5c-380d61499d38",
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<openai.OpenAI at 0x78739c66ee00>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "source": [
        "## Need modification !!!!!"
      ],
      "metadata": {
        "id": "46857b09-9468-48fa-b074-6ea1cb0fbbab"
      },
      "id": "46857b09-9468-48fa-b074-6ea1cb0fbbab",
      "cell_type": "markdown"
    },
    {
      "source": [
        "# Retrieval and Generation Application"
      ],
      "metadata": {
        "id": "afaf3ba3-f849-4191-beec-f4c24cf6b2d9"
      },
      "id": "afaf3ba3-f849-4191-beec-f4c24cf6b2d9",
      "cell_type": "markdown"
    },
    {
      "source": [
        "## Prepare Prompt"
      ],
      "metadata": {
        "id": "f8118feb-1f1a-49ff-94ab-bf13f2376663"
      },
      "id": "f8118feb-1f1a-49ff-94ab-bf13f2376663",
      "cell_type": "markdown"
    },
    {
      "source": [
        "# extraction_prompt = ''' You are a carear consuler who helps job seekers to find their dream jobs, you give professional advice tailored to the need of your client (i.e., job seeker) according to the following information:\n",
        "#     1. Query: Your client's question (enclosed in <query> tag below) that you need to answer\n",
        "#     2. Specification: The job post information (enclosed in <specification> tag below) that might best meets your client's requirements\n",
        "\n",
        "# Upon receiving your aforementioned information, you need to proceed with the following precedures:\n",
        "# Step 1. Analyze your client's abilities, including hard and soft skills.\n",
        "# Step 2. Analyze the skills needed for the best possible jobs in the job specification\n",
        "# Step 3. Summarize your client's strengths that are already sufficient for the job application.\n",
        "# Step 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\n",
        "# Step 5. Finally, give them advice how to get the jobs mentioned in job specification according the reasoning above.\n",
        "\n",
        "# To give your client a professional advice, you MUST give the following feedback:\n",
        "# 1. Job Position: the best possible job position or title you suggest your client to pursue.\n",
        "# 2. Strengths: your client's strengths compared to the job posts\n",
        "# 3. Weaknesses: your client's weaknesses compared to the job posts\n",
        "# 4. Strateries: the methods you suggest to get the jobs mentioned in job posts\n",
        "\n",
        "# FINAL note:\n",
        "# 1. If you cannot find the relevant informaiton in client's question or job specification for your reasoning, just leave it blank (\"\").\n",
        "# 2. Always give advice according to the information given to you (Question and Job Specification), DO NOT make up answer other than those information!\n",
        "\n",
        "# Question:\n",
        "#     <query>{query}</query>\n",
        "# Job Post Information:\n",
        "#     <specification>{specification}</specification>\n",
        "# Advice:\n",
        "# '''"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 50,
        "lastExecutedAt": 1729935914332,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "# extraction_prompt = ''' You are a carear consuler who helps job seekers to find their dream jobs, you give professional advice tailored to the need of your client (i.e., job seeker) according to the following information:\n#     1. Query: Your client's question (enclosed in <query> tag below) that you need to answer\n#     2. Specification: The job post information (enclosed in <specification> tag below) that might best meets your client's requirements\n\n# Upon receiving your aforementioned information, you need to proceed with the following precedures:\n# Step 1. Analyze your client's abilities, including hard and soft skills.\n# Step 2. Analyze the skills needed for the best possible jobs in the job specification\n# Step 3. Summarize your client's strengths that are already sufficient for the job application.\n# Step 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\n# Step 5. Finally, give them advice how to get the jobs mentioned in job specification according the reasoning above.\n\n# To give your client a professional advice, you MUST give the following feedback:\n# 1. Job Position: the best possible job position or title you suggest your client to pursue.\n# 2. Strengths: your client's strengths compared to the job posts\n# 3. Weaknesses: your client's weaknesses compared to the job posts\n# 4. Strateries: the methods you suggest to get the jobs mentioned in job posts \n\n# FINAL note:\n# 1. If you cannot find the relevant informaiton in client's question or job specification for your reasoning, just leave it blank (\"\"). \n# 2. Always give advice according to the information given to you (Question and Job Specification), DO NOT make up answer other than those information!\n\n# Question:\n#     <query>{query}</query>\n# Job Post Information:\n#     <specification>{specification}</specification>\n# Advice:\n# '''",
        "id": "d8f54d12-2ba7-4a80-b189-718f278afab0"
      },
      "id": "d8f54d12-2ba7-4a80-b189-718f278afab0",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "extraction_prompt = ''' You are a carear consuler who helps job seekers to find their dream jobs, you give professional advice tailored to the need of your client (i.e., job seeker) according to the following information:\n",
        "    1. Query: Your client's question (enclosed in <query> tag below) that you need to answer\n",
        "    2. Specification: The job post information (enclosed in <specification> tag below) that might best meets your client's requirements\n",
        "\n",
        "Upon receiving your aforementioned information, you need to proceed with the following precedures:\n",
        "Step 1. Analyze your client's abilities, including hard and soft skills.\n",
        "Step 2. Analyze and summarize the skills needed for the best possible jobs in the job specification\n",
        "Step 3. Summarize your client's strengths that are already sufficient for the job application.\n",
        "Step 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\n",
        "Step 5. Finally, give them advice how to get the jobs mentioned in job specification according the reasoning above.\n",
        "\n",
        "Question:\n",
        "    <query>{query}</query>\n",
        "Job Post Information:\n",
        "    <specification>{specification}</specification>\n",
        "Advice:\n",
        "'''"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1729935914380,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "extraction_prompt = ''' You are a carear consuler who helps job seekers to find their dream jobs, you give professional advice tailored to the need of your client (i.e., job seeker) according to the following information:\n    1. Query: Your client's question (enclosed in <query> tag below) that you need to answer\n    2. Specification: The job post information (enclosed in <specification> tag below) that might best meets your client's requirements\n\nUpon receiving your aforementioned information, you need to proceed with the following precedures:\nStep 1. Analyze your client's abilities, including hard and soft skills.\nStep 2. Analyze and summarize the skills needed for the best possible jobs in the job specification\nStep 3. Summarize your client's strengths that are already sufficient for the job application.\nStep 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\nStep 5. Finally, give them advice how to get the jobs mentioned in job specification according the reasoning above. \n\nQuestion:\n    <query>{query}</query>\nJob Post Information:\n    <specification>{specification}</specification>\nAdvice:\n'''",
        "id": "a398f6e1-caf7-4899-bb7b-52cc3585f009"
      },
      "id": "a398f6e1-caf7-4899-bb7b-52cc3585f009",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Preprare Input Query"
      ],
      "metadata": {
        "id": "6a6dae02-ac78-47c9-8d1c-4bdab15130ee"
      },
      "id": "6a6dae02-ac78-47c9-8d1c-4bdab15130ee",
      "cell_type": "markdown"
    },
    {
      "source": [
        "query = \"I recently graduated with a Bachelor degree in Computer Science, I use Python and have good grades in machine learning and deep learning. I had various projects that allowed me to apply these skills, from building predictive models to analyzing large datasets. I am now seeking an entry-level data scientist or data analyst role.\""
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1729935956604,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "query = \"I recently graduated with a Bachelor degree in Computer Science, I use Python and have good grades in machine learning and deep learning. I had various projects that allowed me to apply these skills, from building predictive models to analyzing large datasets. I am now seeking an entry-level data scientist or data analyst role.\"",
        "id": "d6cd857e-728b-43d8-97b7-bf629dbb0a7f"
      },
      "id": "d6cd857e-728b-43d8-97b7-bf629dbb0a7f",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Search Results based on Query"
      ],
      "metadata": {
        "id": "05a61f4d-ea9f-4d5e-9110-3b64499d82be"
      },
      "id": "05a61f4d-ea9f-4d5e-9110-3b64499d82be",
      "cell_type": "markdown"
    },
    {
      "source": [
        "# results = retriever.invoke(query) #filter={\"source\": \"news\"}\n",
        "# results"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1729935914476,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "# results = retriever.invoke(query) #filter={\"source\": \"news\"}\n# results",
        "id": "1e32707a-7a7a-47e8-83e6-72e446c81789"
      },
      "cell_type": "code",
      "id": "1e32707a-7a7a-47e8-83e6-72e446c81789",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    query , k=5, #filter={\"title\": {\"$in\": keywords}}\n",
        ")\n",
        "i=0\n",
        "specification = \"\"\n",
        "for res, score in results:\n",
        "    print(f\"[{i}][SIM={score:3f}] {res.metadata['title']}\\n---------------------\\n \\\n",
        "          {res.page_content} \\n--------------------\\n \\\n",
        "           [{res.metadata}]\\n\\n\")\n",
        "    specification += ('Title: ' + res.metadata['title'] +'\\n ' + res.page_content)\n",
        "    i+=1"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 109,
        "lastExecutedAt": 1729935959750,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "results = vector_store.similarity_search_with_score(\n    query , k=5, #filter={\"title\": {\"$in\": keywords}}\n)\ni=0\nspecification = \"\"\nfor res, score in results:\n    print(f\"[{i}][SIM={score:3f}] {res.metadata['title']}\\n---------------------\\n \\\n          {res.page_content} \\n--------------------\\n \\\n           [{res.metadata}]\\n\\n\")\n    specification += ('Title: ' + res.metadata['title'] +'\\n ' + res.page_content)\n    i+=1",
        "outputsMetadata": {
          "0": {
            "height": 616,
            "type": "stream"
          }
        },
        "id": "94ef0176-fc23-4ba5-8a26-f1fd341dd51d"
      },
      "id": "94ef0176-fc23-4ba5-8a26-f1fd341dd51d",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "print(specification)"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 53,
        "lastExecutedAt": 1729935978204,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "print(specification)",
        "outputsMetadata": {
          "0": {
            "height": 616,
            "type": "stream"
          }
        },
        "id": "526976e8-2930-4147-a5a4-1380dd76e249",
        "outputId": "a895d59b-88c0-4f70-ff66-b24fccdd1801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "526976e8-2930-4147-a5a4-1380dd76e249",
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "## Get Final Response"
      ],
      "metadata": {
        "id": "14489047-ec44-4f3a-8d77-1ecf13b7a268"
      },
      "id": "14489047-ec44-4f3a-8d77-1ecf13b7a268",
      "cell_type": "markdown"
    },
    {
      "source": [
        "prompt_all = extraction_prompt.format(query=query, specification=specification)\n",
        "print(prompt_all)"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 13,
        "lastExecutedAt": 1729935986235,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "prompt_all = extraction_prompt.format(query=query, specification=specification)\nprint(prompt_all)",
        "outputsMetadata": {
          "0": {
            "height": 458,
            "type": "stream"
          }
        },
        "id": "e23b3f1f-6a18-4433-8164-608250f8e168",
        "outputId": "626dde9e-e210-4859-96cb-88df5f85dce9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "e23b3f1f-6a18-4433-8164-608250f8e168",
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " You are a carear consuler who helps job seekers to find their dream jobs, you give professional advice tailored to the need of your client (i.e., job seeker) according to the following information:\n",
            "    1. Query: Your client's question (enclosed in <query> tag below) that you need to answer\n",
            "    2. Specification: The job post information (enclosed in <specification> tag below) that might best meets your client's requirements\n",
            "\n",
            "Upon receiving your aforementioned information, you need to proceed with the following precedures:\n",
            "Step 1. Analyze your client's abilities, including hard and soft skills.\n",
            "Step 2. Analyze and summarize the skills needed for the best possible jobs in the job specification\n",
            "Step 3. Summarize your client's strengths that are already sufficient for the job application.\n",
            "Step 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\n",
            "Step 5. Finally, give them advice how to get the jobs mentioned in job specification according the reasoning above. \n",
            "\n",
            "Question:\n",
            "    <query>I recently graduated with a Bachelor degree in Computer Science, I use Python and have good grades in machine learning and deep learning. I had various projects that allowed me to apply these skills, from building predictive models to analyzing large datasets. I am now seeking an entry-level data scientist or data analyst role.</query>\n",
            "Job Post Information:\n",
            "    <specification></specification>\n",
            "Advice:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import tiktoken\n",
        "\n",
        "# Define a function to count tokens for a given prompt and model\n",
        "def count_tokens(text, model=\"gpt-3.5-turbo-instruct\"):\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "# Count the number of tokens in the prompt\n",
        "prompt_tokens = count_tokens(prompt_all);print(f\"total prompt tokens = {prompt_tokens}\")\n",
        "\n",
        "# Token limit for gpt-3.5-turbo-instruct\n",
        "token_limit = 4097\n",
        "\n",
        "# Ensure the total tokens (prompt + response) is within the limit\n",
        "# Assume you want the model to generate a maximum of 1000 tokens in the response\n",
        "response_max_tokens = 1000\n",
        "if prompt_tokens + response_max_tokens > token_limit:\n",
        "    print('total token size exceeds limit, start trimming!')\n",
        "    # Calculate the allowable prompt length\n",
        "    max_prompt_tokens = token_limit - response_max_tokens\n",
        "\n",
        "    # Trim the prompt to fit within the token limit\n",
        "    trimmed_prompt = prompt_all[:max_prompt_tokens]\n",
        "\n",
        "    # Notify user about trimming\n",
        "    print(f\"Prompt trimmed from {prompt_tokens} to {max_prompt_tokens} tokens.\")\n",
        "    print(\"final prompt_all:\\n\",prompt_all)\n",
        "\n",
        "    # Update the prompt with the trimmed version\n",
        "    prompt_all = trimmed_prompt\n",
        "else:\n",
        "    print('total token size doesn\\'t  exceeds limit, good job!')\n",
        "\n"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 13,
        "lastExecutedAt": 1729935997918,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "import tiktoken\n\n# Define a function to count tokens for a given prompt and model\ndef count_tokens(text, model=\"gpt-3.5-turbo-instruct\"):\n    encoding = tiktoken.encoding_for_model(model)\n    return len(encoding.encode(text))\n\n# Count the number of tokens in the prompt\nprompt_tokens = count_tokens(prompt_all);print(f\"total prompt tokens = {prompt_tokens}\")\n\n# Token limit for gpt-3.5-turbo-instruct\ntoken_limit = 4097\n\n# Ensure the total tokens (prompt + response) is within the limit\n# Assume you want the model to generate a maximum of 1000 tokens in the response\nresponse_max_tokens = 1000\nif prompt_tokens + response_max_tokens > token_limit:\n    print('total token size exceeds limit, start trimming!')\n    # Calculate the allowable prompt length\n    max_prompt_tokens = token_limit - response_max_tokens\n\n    # Trim the prompt to fit within the token limit\n    trimmed_prompt = prompt_all[:max_prompt_tokens]\n\n    # Notify user about trimming\n    print(f\"Prompt trimmed from {prompt_tokens} to {max_prompt_tokens} tokens.\")\n    print(\"final prompt_all:\\n\",prompt_all)\n\n    # Update the prompt with the trimmed version\n    prompt_all = trimmed_prompt\nelse:\n    print('total token size doesn\\'t  exceeds limit, good job!')\n\n",
        "outputsMetadata": {
          "0": {
            "height": 59,
            "type": "stream"
          }
        },
        "id": "970a2dea-77d9-4364-b5f0-e9556f9e616a",
        "outputId": "0062bf94-041b-4b94-bdc8-09cf388f2474",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "970a2dea-77d9-4364-b5f0-e9556f9e616a",
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total prompt tokens = 300\n",
            "total token size doesn't  exceeds limit, good job!\n"
          ]
        }
      ]
    },
    {
      "source": [
        "response = client.completions.create(model=\"gpt-3.5-turbo-instruct\",\n",
        "                                     prompt=prompt_all,\n",
        "                                     max_tokens=response_max_tokens)\n",
        "print(response.choices[0].text)"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 5187,
        "lastExecutedAt": 1729936007254,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "response = client.completions.create(model=\"gpt-3.5-turbo-instruct\",  \n                                     prompt=prompt_all,\n                                     max_tokens=response_max_tokens) \nprint(response.choices[0].text)",
        "outputsMetadata": {
          "0": {
            "height": 616,
            "type": "stream"
          }
        },
        "id": "2e7980fe-46db-48c4-ae60-b12a5da8bdbf",
        "outputId": "4be00213-921d-4042-956f-ab274b5d2363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2e7980fe-46db-48c4-ae60-b12a5da8bdbf",
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1. Analyze your client's abilities, including hard and soft skills.\n",
            "Based on the information provided, your client has a Bachelor's degree in Computer Science with a focus on data analysis and machine learning. They have strong skills in Python, including building predictive models and analyzing large datasets. They also have soft skills such as problem-solving and critical thinking.\n",
            "\n",
            "Step 2. Analyze and summarize the skills needed for the best possible jobs in the job specification\n",
            "From the job specification, the desired skills for an entry-level data scientist or data analyst position include knowledge of programming languages such as Python and experience with machine learning and data analysis. They are also looking for candidates with good communication and problem-solving skills.\n",
            "\n",
            "Step 3. Summarize your client's strengths that are already sufficient for the job application.\n",
            "Your client's strengths align with the skills listed in the job specification. They have a strong foundation in Python and experience with machine learning and data analysis, which makes them a suitable candidate for the position. Their soft skills, such as problem-solving and critical thinking, are also desirable qualities for the job.\n",
            "\n",
            "Step 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\n",
            "Your client might need to improve their skills in other programming languages besides Python, such as R or SQL, to have a well-rounded skillset for the job. They could also gain more experience in data visualization and communication to better meet the job requirements.\n",
            "\n",
            "Step 5. Finally, give them advice how to get the jobs mentioned in job specification according the reasoning above.\n",
            "To increase their chances of getting the job, your client can focus on showcasing their experience with Python, machine learning, and data analysis in their resume and cover letter. They can also work on improving their skills in other programming languages and gaining experience in data visualization through online courses or personal projects. Additionally, they should highlight their communication and problem-solving skills in the job application process. Networking and attending industry events can also help them make valuable connections and potential job opportunities.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# What If: Generation without Application"
      ],
      "metadata": {
        "id": "896a2613-5202-479a-802f-2d5dcd493e94"
      },
      "cell_type": "markdown",
      "id": "896a2613-5202-479a-802f-2d5dcd493e94"
    },
    {
      "source": [
        "extraction_prompt = ''' You are a carear consuler who helps job seekers to find their dream jobs, you give professional advice tailored to the need of your client (i.e., job seeker) according to the following information:\n",
        "    1. Query: Your client's question (enclosed in <query> tag below) that you need to answer\n",
        "\n",
        "\n",
        "Upon receiving your aforementioned information, you need to proceed with the following precedures:\n",
        "Step 1. Analyze your client's abilities, including hard and soft skills.\n",
        "Step 2. Analyze and summarize the skills needed for the best possible jobs\n",
        "Step 3. Summarize your client's strengths that are already sufficient for the job application.\n",
        "Step 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\n",
        "Step 5. Finally, give them advice how to get the jobs.\n",
        "\n",
        "Question:\n",
        "    <query>{query}</query>\n",
        "\n",
        "Advice:\n",
        "'''\n",
        "\n",
        "prompt_all = extraction_prompt.format(query=query)"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 51,
        "lastExecutedAt": 1729936036272,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "extraction_prompt = ''' You are a carear consuler who helps job seekers to find their dream jobs, you give professional advice tailored to the need of your client (i.e., job seeker) according to the following information:\n    1. Query: Your client's question (enclosed in <query> tag below) that you need to answer\n\n\nUpon receiving your aforementioned information, you need to proceed with the following precedures:\nStep 1. Analyze your client's abilities, including hard and soft skills.\nStep 2. Analyze and summarize the skills needed for the best possible jobs\nStep 3. Summarize your client's strengths that are already sufficient for the job application.\nStep 4. Summarize your client's weaknesses that they need to improve in order to meet the job requirements.\nStep 5. Finally, give them advice how to get the jobs.\n\nQuestion:\n    <query>{query}</query>\n\nAdvice:\n'''\n\nprompt_all = extraction_prompt.format(query=query)",
        "outputsMetadata": {
          "0": {
            "height": 563,
            "type": "stream"
          }
        },
        "id": "90e5f89d-998e-4d94-95af-63db4312c944"
      },
      "cell_type": "code",
      "id": "90e5f89d-998e-4d94-95af-63db4312c944",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "response = client.completions.create(model=\"gpt-3.5-turbo-instruct\",\n",
        "                                     prompt=prompt_all,\n",
        "                                     max_tokens=response_max_tokens)\n",
        "print(response.choices[0].text)"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 2459,
        "lastExecutedAt": 1729936047520,
        "lastExecutedByKernel": "a89a7bc1-1549-402a-bc95-5aac60bc1893",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "response = client.completions.create(model=\"gpt-3.5-turbo-instruct\",  \n                                     prompt=prompt_all,\n                                     max_tokens=response_max_tokens) \nprint(response.choices[0].text)",
        "outputsMetadata": {
          "0": {
            "height": 616,
            "type": "stream"
          }
        },
        "id": "bfb7229b-7156-40d7-bd5e-ae5972553502",
        "outputId": "f1c8564a-2a45-4819-f8ce-03069184ec35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "id": "bfb7229b-7156-40d7-bd5e-ae5972553502",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After analyzing your abilities, I can say that you have a strong foundation for a career in data science. With your Bachelor degree in Computer Science and your proficiency in Python, as well as your good grades in machine learning and deep learning, you possess the necessary technical skills for an entry-level data scientist or data analyst role.\n",
            "\n",
            "To increase your chances of landing your dream job, I would suggest improving your soft skills such as communication, problem-solving, and teamwork. These skills are highly valued in the data science industry and will make you stand out among other candidates.\n",
            "\n",
            "Additionally, you can continue to enhance your technical skills by taking online courses or participating in coding workshops to stay updated with the latest technologies and techniques in data science.\n",
            "\n",
            "I would also recommend networking and attending data science events or conferences to expand your professional connections and gain insights into the industry.\n",
            "\n",
            "Finally, when applying for jobs, make sure to tailor your resume and cover letter to each specific role and highlight your relevant experiences and skills. And don't forget to practice for interviews and showcase your passion and knowledge in data science.\n",
            "\n",
            "With your determination and continuous effort, I am confident that you will eventually land your desired data science job. Best of luck in your job search!\n"
          ]
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "editor": "DataLab",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}